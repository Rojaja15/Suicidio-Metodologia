# Cluster

# Limpieza

## Librerias

```{r}

library(tidyverse)
library(fastDummies)
library(FactoMineR)


library(factoextra)
library(cluster)
library(dendextend)
library(caret)
library(reticulate)
library(htmltools)
library(IRdisplay)
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)
library(psych)
library(ggcorrplot)
library(janitor)
library(summarytools)
library(broom)
library(readxl)
library(cowplot)
library(reticulate)

```

## Funciones

```{r}

inercia_intraclase <- function(datos, grupos) {
  # Combina los datos y los grupos en una sola tabla
  tabla <- cbind(datos, grupos)

  # Calcula los centroides (puntos medios) de cada grupo
  centroides <- lapply(unique(grupos), function(id_grupo) {
    subconjunto <- subset(tabla, grupos == id_grupo)
    apply(subconjunto[, 1:(ncol(subconjunto)-1)], 2, mean)
  })

  # Calcula la suma de las distancias al cuadrado de cada punto a su centroide
  suma_cuadrados <- sum(sapply(unique(grupos), function(id_grupo) {
    subconjunto <- subset(tabla, grupos == id_grupo, select = -grupos)
    centroide_actual <- centroides[[id_grupo]]

    # Resta el centroide a cada punto y calcula la suma de cuadrados
    sum(sweep(subconjunto, 2, centroide_actual, "-")^2)
  }))

  return(suma_cuadrados)
}

```

```{r}

options(tibble.width = Inf)

```

```{r}

datos_crudos <- read_excel("Datos.xlsx")

```

```{r}

head(datos_crudos)

```


```{r}
df_escalado <- read.csv("Data/df_escalado.csv")
```

# Cluster Jerarquico

Hclust: Grupos mas homogeneos

```{r}

distancias_hclust <- c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski")
agregaciones_hclust <- c("ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid")

```

Datra frame vacio para guardar los datos

```{r}

resultados_hclust <- data.frame(
  metodo_distancia = character(),
  metodo_agregacion = character(),
  inercia = numeric(),
  stringsAsFactors = FALSE
)

```

**Ciclo for** para ver cuál es el mejor modelo.

```{r}

# 5. Bucle para probar cada combinación
for (distancia in distancias_hclust) {
  try({
    matriz_distancia <- dist(df_escalado, method = distancia)

    for (agregacion in agregaciones_hclust) {
      # Evitar combinaciones inválidas
      if ((agregacion %in% c("ward.D", "ward.D2", "centroid", "median")) && (distancia != "euclidean")) {
        next
      }

      # Construir el modelo y cortar el árbol en k=3 clústeres
      modelo <- hclust(matriz_distancia, method = agregacion)
      grupos <- cutree(modelo, k = 3)

      # Calcular la inercia y guardarla
      inercia_actual <- inercia_intraclase(df_escalado, grupos)

      resultados_hclust <- resultados_hclust %>%
        add_row(
          metodo_distancia = distancia,
          metodo_agregacion = agregacion,
          inercia = inercia_actual
        )
    }
  }, silent = TRUE)
}

```

```{r}

mejores_hclust <- resultados_hclust %>% arrange(inercia)
print("--- Ranking de HCLUST (menor inercia es mejor) ---")
print(mejores_hclust)

```

## Visualizacion

```{r}

dist_euclidean <- dist(df_escalado, method = "euclidean")
hclust_ward <- hclust(dist_euclidean, method = "ward.D")

dend_obj <- as.dendrogram(hclust_ward)
color_dend <- color_branches(dend_obj, h = 42,
                             groupLabels =TRUE)
plot(color_dend)

```

Agnes: Escrutura mas natual y clara

```{r}

distancias_agnes <- c("euclidean", "manhattan") # agnes es más sensible a algunas distancias
agregaciones_agnes <- c("ward", "single", "complete", "average")

```

Crear df para guardar resultados

```{r}

resultados_agnes <- data.frame(
  metodo_distancia = character(),
  metodo_agregacion = character(),
  coeficiente_aglomerativo = numeric(),
  stringsAsFactors = FALSE
)

```

Crear **ciclo for**

```{r}

for (distancia in distancias_agnes) {
  try({
    matriz_distancia <- dist(df_escalado, method = distancia)

    for (agregacion in agregaciones_agnes) {
      # Evitar combinaciones inválidas
      if (agregacion == "ward" && distancia != "euclidean") {
        next
      }

      # Construir el modelo con agnes
      modelo <- agnes(matriz_distancia, method = agregacion)

      # Guardar el Coeficiente Aglomerativo
      resultados_agnes <- resultados_agnes %>%
        add_row(
          metodo_distancia = distancia,
          metodo_agregacion = agregacion,
          coeficiente_aglomerativo = modelo$ac
        )
    }
  }, silent = TRUE)
}

```

```{r}

mejores_agnes <- resultados_agnes %>% arrange(desc(coeficiente_aglomerativo))
print("--- Ranking de AGNES (mayor coeficiente es mejor) ---")
print(mejores_agnes)

```

Visualizacion

```{r}

# dist_euclidean <- dist(df_escalado, method = "euclidean")
# hclust_complete <- hclust(dist_manhattan, method = "ward.D2")

# dend_obj <- as.dendrogram(hclust_complete)
# color_dend <- color_branches(dend_obj, h = 34,
                             # groupLabels =TRUE)
# plot(color_dend)

```

## Visualizacion con python

```{python}

# 2. Configurar la ruta de Python

# use_python("/usr/bin/python3")
use_python("C:/Python39/python.exe")   # Windows

# 3. Preparar las variables de R
dist_final <- dist(df_escalado, method = "euclidean")
modelo_final_hclust <- hclust(dist_final, method = "ward.D2")
grupos_cluster <- cutree(modelo_final_hclust, k = 3)

# 4. Ejecutar el bloque de código de Python para CREAR el HTML del gráfico
py_run_string(r"(
# Importar librerías
import pandas as pd
from sklearn.decomposition import PCA
import plotly.express as px

# Acceder a los objetos de R
df_python = r.df_escalado
clusters_r = r.grupos_cluster

# Realizar ACP
pca = PCA(n_components=3)
componentes = pca.fit_transform(df_python)

# Crear dataframe con los componentes y clústeres
df_pca = pd.DataFrame(
    data=componentes,
    columns=['Componente Principal 1', 'Componente Principal 2', 'Componente Principal 3']
)
df_pca['cluster'] = clusters_r
df_pca['cluster'] = df_pca['cluster'].astype(str)

# Crear el objeto del gráfico
fig = px.scatter_3d(
    df_pca,
    x='Componente Principal 1',
    y='Componente Principal 2',
    z='Componente Principal 3',
    color='cluster',
    title='Análisis de Clústeres en 3D (con Python)',
    labels={'cluster': 'Clúster'}
)

# NO USAMOS fig.show(). En su lugar, convertimos el gráfico a HTML.
# Esta variable 'grafico_html_str' estará disponible en R.
grafico_html_str = fig.to_html()
)")

# 5. En R, acceder al HTML creado por Python y mostrarlo
#    py$grafico_html_str trae la variable de Python a R.
#    display_html() la renderiza en Colab.
display_html(py$grafico_html_str)

```


## AFC

```{r}

df <- df %>%
  mutate(
    EIS_Cat = case_when(
      EIS_Roberts <= 5          ~ "Bajo",
      EIS_Roberts >= 6 & EIS_Roberts <= 8 ~ "Medio",
      EIS_Roberts >= 9          ~ "Alto",
      TRUE                      ~ NA_character_ # Manejo de casos inesperados (como NAs)
    )
  )

```


```{r}

df_categoricas <- df[, c("medicamento_psiquiatrico", "discapacidad_fisica",
                         "suicidio_familiar","ruptura_reciente", "crisis_pareja_actual",
                         "medidas_cautelares","estado_civil","EIS_Cat")]

```

```{r}

columnas_a_factor <- c(
  "medicamento_psiquiatrico_No", "medicamento_psiquiatrico_Sí",
  "discapacidad_fisica_No", "discapacidad_fisica_Sí",
  "suicidio_familiar_No", "suicidio_familiar_Sí",
  "ruptura_reciente_No", "ruptura_reciente_Sí",
  "crisis_pareja_actual_No", "crisis_pareja_actual_Sí",
  "medidas_cautelares_No",
  "posgrado", "sec_completa", "sec_incompleta", "tecnico",
  "u_completo", "u_incompleto",
  "casado", "divorciado", "separado", "soltero", "union_libre",
  "desempleado", "asalariado", "independiente",
  "heterosexual", "sexualidad_otra",
  "EIS_alto", "EIS_medio", "EIS_bajo"
)

# 2. Extraer las columnas y convertirlas a factor en un nuevo data frame
# Asegúrate de que los nombres en el vector coincidan exactamente con tu data frame
df_factores <- df_escalado %>%
  select(all_of(columnas_a_factor)) %>%
  mutate(across(everything(), as.factor))

# 3. (Opcional) Verificar que la conversión fue exitosa
# Deberías ver que todas las columnas en df_factores son de tipo 'Factor'
str(df_factores)

```

```{r}

AFC <- MCA(df_categoricas, graph = FALSE)

```

```{r}

fviz_screeplot(AFC, addlabels = TRUE, ylim = c(0, 50))

```

```{r}

fviz_mca_var(AFC, repel = TRUE,
             cex.var = 6,   # Aumenta el tamaño de los puntos
             labelsize = 2,
             ggtheme = theme_minimal())

```

## Misc

```{r}

# 2. Ejecutar el bloque de código de Python para CREAR el HTML del gráfico
py_run_string(r"(
# Importar librerías
import pandas as pd
from sklearn.decomposition import PCA
import plotly.graph_objects as go

# Acceder al dataframe escalado de R
df_python = r.df_seleccionado

# Realizar ACP
pca = PCA(n_components=3)
pca.fit(df_python)

# Obtener los "loadings"
loadings = pca.components_.T
variable_names = df_python.columns

# Crear un DataFrame con los loadings
df_loadings = pd.DataFrame(
    loadings,
    columns=['PC1', 'PC2', 'PC3'],
    index=variable_names
)

# --- Crear el gráfico 3D con Plotly Graph Objects ---
fig = go.Figure()

# 1. AÑADIR LÍNEAS Y TEXTO JUNTOS
for i, var in enumerate(df_loadings.index):
    pc1_val = df_loadings['PC1'].iloc[i]
    pc2_val = df_loadings['PC2'].iloc[i]
    pc3_val = df_loadings['PC3'].iloc[i]

    # --- LÓGICA DE COLOR Y GROSOR ---
    # Define tu lista de variables especiales
    variables_eis = ['EIS_bajo', 'EIS_medio', 'EIS_alto', 'EIS_Roberts']

    # Asigna el color y el grosor basado en si 'var' está en la lista
    if var in variables_eis:
        color_vector = 'orange'
        vector_width = 8  # <-- EL DOBLE DE GRUESO (8)
    else:
        color_vector = 'steelblue'
        vector_width = 4  # <-- GROSOR BASE (4)
    # ------------------------------------------

    fig.add_trace(go.Scatter3d(
        x=[0, pc1_val],
        y=[0, pc2_val],
        z=[0, pc3_val],
        mode='lines+text',
        text=["", var],
        textposition='middle right',

        # USA LAS VARIABLES DE COLOR Y GROSOR
        line=dict(color=color_vector, width=vector_width),

        name=var
    ))

# 3. Configurar el layout (título, ejes, etc.)
fig.update_layout(
    title='Círculo de Correlaciones 3D (Haz clic en la leyenda)',
    scene=dict(
        xaxis=dict(title='', showticklabels=False),
        yaxis=dict(title='', showticklabels=False),
        zaxis=dict(title='', showticklabels=False),
        aspectmode='data'
    ),
    margin=dict(l=0, r=0, b=0, t=40)
)

# Convertimos el gráfico a HTML.
grafico_html_str = fig.to_html()
)")

# 3. En R, acceder al HTML creado por Python y mostrarlo
display_html(py$grafico_html_str)

```

```{r}

# 2. Ejecutar el bloque de código de Python para CREAR el HTML del gráfico
py_run_string(r"(
# Importar librerías
import pandas as pd
from sklearn.decomposition import PCA
import plotly.graph_objects as go

# Acceder al dataframe escalado de R
df_python = r.df_seleccionado

# Realizar ACP
pca = PCA(n_components=3)
scores = pca.fit_transform(df_python) # Obtenemos scores
loadings = pca.components_.T # Obtenemos loadings
variable_names = df_python.columns

# Crear un DataFrame con los loadings
df_loadings = pd.DataFrame(
    loadings,
    columns=['PC1', 'PC2', 'PC3'],
    index=variable_names
)

# --- ¡MODIFICACIÓN AQUÍ! ---
# Define un factor de escala. Puedes ajustar este número (p.ej., 5, 10, 20)
# para hacer los vectores más largos o cortos.
escala_vector = 10
# --------------------------

# --- Crear el gráfico 3D con Plotly Graph Objects ---
fig = go.Figure()

# 1. AÑADIR LÍNEAS Y TEXTO JUNTOS (Vectores de variables)
for i, var in enumerate(df_loadings.index):

    # --- ¡MODIFICACIÓN AQUÍ! ---
    # Multiplicamos las coordenadas del vector por el factor de escala
    pc1_val = df_loadings['PC1'].iloc[i] * escala_vector
    pc2_val = df_loadings['PC2'].iloc[i] * escala_vector
    pc3_val = df_loadings['PC3'].iloc[i] * escala_vector
    # --------------------------

    # --- LÓGICA DE COLOR Y GROSOR (igual) ---
    variables_eis = ['EIS_bajo', 'EIS_medio', 'EIS_alto', 'EIS_Roberts']
    if var in variables_eis:
        color_vector = 'orange'
        vector_width = 8
    else:
        color_vector = 'steelblue'
        vector_width = 4
    # ------------------------------------------

    # Esta parte usa los 'pcN_val' que AHORA SÍ están escalados
    fig.add_trace(go.Scatter3d(
        x=[0, pc1_val],
        y=[0, pc2_val],
        z=[0, pc3_val],
        mode='lines+text',
        text=["", var],
        textposition='middle right',
        line=dict(color=color_vector, width=vector_width),
        name=var,
        legendgroup='variables',
        showlegend=False
    ))

# 2. AÑADIR LOS INDIVIDUOS COMO PUNTOS (Esto queda igual)
fig.add_trace(go.Scatter3d(
    x=scores[:, 0], # Score PC1 de cada individuo
    y=scores[:, 1], # Score PC2 de cada individuo
    z=scores[:, 2], # Score PC3 de cada individuo
    mode='markers',
    name='Individuos',
    marker=dict(
        size=6,
        color='red',
        opacity=0.1
    )
))

# 3. Configurar el layout (título, ejes, etc.)
fig.update_layout(
    title='Biplot 3D (Vectores Escalados)', # Título actualizado
    scene=dict(
        xaxis=dict(title='PC1', showticklabels=True),
        yaxis=dict(title='PC2', showticklabels=True),
        zaxis=dict(title='PC3', showticklabels=True),
        aspectmode='data'
    ),
    margin=dict(l=0, r=0, b=0, t=40)
)

# Convertimos el gráfico a HTML.
grafico_html_str = fig.to_html()
)")

# 3. En R, acceder al HTML creado por Python y mostrarlo
display_html(py$grafico_html_str)

```

```{r}

# --- 1. Definir la variable objetivo ---
target_var <- "eis_roberts"

# --- 2. Lista de variables a excluir ---
vars_a_excluir <- c(
  "roberts_pensado_matarme",
  "roberts_mejor_sin_mi",
  "roberts_pensado_muerte",
  "roberts_plan_suicida"
)

# --- 3. Encontrar todas las variables numéricas ---
all_numeric_vars <- names(df)[sapply(df, is.numeric)]

# --- 4. Excluir la variable objetivo Y las variables de la lista ---
vars_to_correlate <- setdiff(all_numeric_vars, target_var)
vars_to_correlate <- setdiff(vars_to_correlate, vars_a_excluir) # <- Esta es la línea nueva

# --- 5. Calcular las correlaciones usando sapply ---
correlations <- sapply(vars_to_correlate, function(var_name) {
  cor(df[[target_var]], df[[var_name]], use = "pairwise.complete.obs")
})

# --- 6. Crear el dataframe final ---
cor_df_filtrado <- data.frame(
  nombre_variable = names(correlations),
  correlacion_pearson = correlations,
  row.names = NULL
)

# --- 7. Ordenar por la correlación más fuerte ---
cor_df_filtrado <- cor_df_filtrado[order(abs(cor_df_filtrado$correlacion_pearson), decreasing = TRUE), ]

# --- 8. Ver el resultado ---
print(cor_df_filtrado)

```

```{r}

# --- 1. Definir listas de variables ---
# Las 3 variables dummy que son ahora el objetivo
target_dummies <- c("EIS_bajo", "EIS_medio", "EIS_alto")

# Lista de variables a excluir (basado en solicitudes anteriores)
vars_a_excluir <- c(
  "roberts_pensado_matarme",
  "roberts_mejor_sin_mi",
  "roberts_pensado_muerte",
  "roberts_plan_suicida",
  "eis_roberts"
)

# --- 2. Identificar variables numéricas a contrastar ---
# Asume que tu dataframe se llama 'df_escalado'
all_numeric_vars <- names(df_escalado)[sapply(df_escalado, is.numeric)]

# Filtramos para quedarnos solo con las numéricas que nos interesan
numeric_vars <- setdiff(all_numeric_vars, c(target_dummies, vars_a_excluir))

# --- 3. Calcular la matriz de correlaciones ---
# Usamos 'sapply' anidado:
# El 'sapply' externo itera sobre las variables numéricas
# El 'sapply' interno itera sobre las 3 dummies para cada variable numérica
cor_matrix <- sapply(numeric_vars, function(var_num) {

  sapply(target_dummies, function(dummy_name) {
    cor(df_escalado[[var_num]], df_escalado[[dummy_name]],
        use = "pairwise.complete.obs")
  })

})

# --- 4. Transponer y convertir a dataframe ---
# La matriz resultante está "tumbada", la transponemos (t())
cor_df_dummies <- as.data.frame(t(cor_matrix))

# --- 5. Limpiar y formatear el dataframe ---
# Los nombres de las variables están como nombres de fila, los pasamos a una columna
cor_df_dummies$nombre_variable <- rownames(cor_df_dummies)
rownames(cor_df_dummies) <- NULL

# Reordenar las columnas para que 'nombre_variable' esté primero
cor_df_dummies <- cor_df_dummies[, c("nombre_variable", target_dummies)]

# --- 6. Ordenar el dataframe final ---
# Ordenado por 'EIS_bajo' de mayor a menor.
# Puedes cambiar 'EIS_bajo' por 'EIS_medio' o 'EIS_alto' si lo prefieres.
cor_df_ordenado <- cor_df_dummies[
  order(cor_df_dummies$EIS_bajo, decreasing = TRUE),
]

# --- 7. Ver el resultado ---
print( cor_df_ordenado[order(cor_df_ordenado$EIS_medio, decreasing = TRUE) , c("nombre_variable", "EIS_alto")])

```

## Creación de los clústers finales

```{r}
df <- read.csv("Data/df.csv")
```


```{r}
variables_filtradas <- c(
  "ocultar_emociones", "dificultad_pedir_ayuda", "sin_motivacion", "preocupado",            
  "sin_proposito", "no_reconocer_logros", "incapaz_solucionar", "intento_autolesion",    
  "plan_autolesion", "autolesion_fisica", "conduccion_temeraria", "soledad",               
  "conflicto_familiar", "EIS_Roberts"
)
```

```{r}
df_cluster <- df_escalado %>%
  select(all_of(variables_filtradas))
```

```{r}
distancias_hclust <- c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski")
agregaciones_hclust <- c("ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid")

# 2. Data frame para guardar resultados
resultados_hclust <- data.frame(
  metodo_distancia = character(),
  metodo_agregacion = character(),
  inercia = numeric(),
  stringsAsFactors = FALSE
)
```


```{r}
for (distancia in distancias_hclust) {
  try({
    # ----- USA EL DF FILTRADO -----
    matriz_distancia <- dist(df_cluster, method = distancia) 

    for (agregacion in agregaciones_hclust) {
      if ((agregacion %in% c("ward.D", "ward.D2", "centroid", "median")) && (distancia != "euclidean")) {
        next
      }

      modelo <- hclust(matriz_distancia, method = agregacion)
      grupos <- cutree(modelo, k = 3) # Asumiendo k=3

      # ----- USA EL DF FILTRADO -----
      inercia_actual <- inercia_intraclase(df_cluster, grupos)

      resultados_hclust <- resultados_hclust %>%
        add_row(
          metodo_distancia = distancia,
          metodo_agregacion = agregacion,
          inercia = inercia_actual
        )
    }
  }, silent = TRUE)
}
```

```{r}
mejores_hclust <- resultados_hclust %>% arrange(inercia)
print("--- Ranking de HCLUST (menor inercia es mejor) ---")
print(head(mejores_hclust))
```

```{r}
dist_ganadora <- mejores_hclust$metodo_distancia[1]
agregacion_ganadora <- mejores_hclust$metodo_agregacion[1]
```

```{r}
# 3. Calcular y visualizar el dendrograma del modelo ganador
matriz_dist_ganadora <- dist(df_cluster, method = dist_ganadora)
modelo_ganador <- hclust(matriz_dist_ganadora, method = agregacion_ganadora)

# 4. Crear y colorear el dendrograma (k=3)
dend_obj <- as.dendrogram(modelo_ganador)
color_dend <- color_branches(dend_obj, k = 3) # Colorear 3 clústeres

plot(color_dend, 
     main = paste("Dendrograma (Filtrado) -", dist_ganadora, "+", agregacion_ganadora),
     leaflab = "none") # Oculta etiquetas de hojas para que sea legible
```


```{r}
# 1. Calcular la matriz de distancia ganadora (basada en el df filtrado)
matriz_dist_ganadora <- dist(df_cluster, method = dist_ganadora)

# 2. Calcular el modelo de clustering jerárquico final
modelo_ganador <- hclust(matriz_dist_ganadora, method = agregacion_ganadora)

# 3. Definir el número de clústeres (k=3)
k <- 3

# 4. ¡AQUÍ ESTÁ LA CLAVE!
# Usar cutree() para cortar el árbol y generar el vector de clústeres
clusters_hc <- cutree(modelo_ganador, k = k)

# 5. (Opcional) Convertir el vector a factor para facilitar análisis futuros
clusters_hc_factor <- factor(clusters_hc)



print("Conteo de observaciones por clúster:")
print(table(clusters_hc_factor))
```

```{r}
# --- PASO 0: Asegurarnos de tener los objetos correctos ---
# Reafirmar las 14 variables seleccionadas (si no lo has hecho ya en esta sesión)
variables_filtradas <- c(
  "ocultar_emociones", "dificultad_pedir_ayuda", "sin_motivacion", "preocupado",            
  "sin_proposito", "no_reconocer_logros", "incapaz_solucionar", "intento_autolesion",    
  "plan_autolesion", "autolesion_fisica", "conduccion_temeraria", "soledad",               
  "conflicto_familiar", "EIS_Roberts"
)

# Recrear df_cluster (si no está en tu entorno actual)
df_cluster <- df_escalado %>%
  select(all_of(variables_filtradas))

# Recrear el modelo de clustering jerárquico final (si no está en tu entorno actual)
# Asume que 'dist_ganadora' y 'agregacion_ganadora' ya están definidos por el código anterior
matriz_dist_ganadora <- dist(df_cluster, method = dist_ganadora)
modelo_ganador <- hclust(matriz_dist_ganadora, method = agregacion_ganadora)
k <- 3 # Número de clústeres
clusters_hc_factor <- factor(cutree(modelo_ganador, k = k))

# --- PASO 1: Preparar el dataframe para el ACP con la información del clúster ---
# Es crucial que df_cluster tenga la misma cantidad y orden de filas que clusters_hc_factor
df_cluster_con_cluster <- df_cluster
df_cluster_con_cluster$cluster <- clusters_hc_factor

# --- PASO 2: Realizar el ACP sobre las variables filtradas ---
# Si ya lo tienes hecho como 'acp_filtrado', puedes saltar este paso y usar ese.
# Si no, lo volvemos a calcular sobre 'df_cluster'
acp_para_cluster <- prcomp(df_cluster, scale. = FALSE) 
# NOTA: Usamos 'df_cluster' (sin la columna 'cluster') para calcular el ACP.
# La columna 'cluster' se usa SOLO para el COLOR en la visualización.

# --- PASO 3: Generar el gráfico de individuos coloreados por clúster ---
# 1. Definir la paleta de colores personalizada
colores_clusters <- c("turquoise", "steelblue", "tomato")

# 2. Generar el gráfico de individuos (sin etiquetas) con colores personalizados
fviz_pca_ind(acp_para_cluster,
             geom = "point",                           # <--- CAMBIO: Solo muestra puntos, no texto
             col.ind = df_cluster_con_cluster$cluster, # Colorear por la variable 'cluster'
             palette = colores_clusters,               # <--- CAMBIO: Paleta personalizada
             addEllipses = TRUE,                       # Añadir elipses de confianza
             ellipse.type = "convex",                  
             legend.title = "Clúster",                 
             pointsize = 3) +                         # Aumenté un poco el tamaño del punto
  theme_minimal() +
  labs(title = "ACP de Individuos (Coloreados por Clúster)") + # Título
  theme(plot.title = element_text(hjust = 0.5))       # Centrar título
```

```{r}
# 1. Paleta de colores (la misma de antes)
colores_clusters <- c("turquoise", "steelblue", "tomato")

# 2. Generar el biplot (Versión Corregida)
fviz_pca_biplot(acp_para_cluster,
                # --- Control de Individuos (Se mantiene igual) ---
                geom.ind = "point",                         
                col.ind = df_cluster_con_cluster$cluster,   # Mapeo de color #1 (Discreto)
                palette = colores_clusters,               # Escala para el mapeo #1
                pointsize = 2.5,                            
                alpha.ind = 0.6,                            
                
                # --- Control de Variables (¡AQUÍ EL CAMBIO!) ---
                col.var = "grey40",  # <--- Asignamos un color fijo, no un mapeo
                repel = TRUE,                               
                
                # --- Opciones Generales ---
                addEllipses = TRUE,                         
                ellipse.type = "convex",
                legend.title = "Clúster",
                title = "Biplot ACP (Individuos por Clúster y Variables)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

# Análisis factorial de datos mixtos

```{r}
vars_admin_excluir <- c("fecha", "dia", "provincia", "canton", "grupo_wem")
df_para_afc <- df %>%
  select(-all_of(vars_admin_excluir)) %>%
  select(where(is.character) | where(is.factor)) %>%
  mutate(across(everything(), as.factor))
```

```{r}
# 1. Correr el AFC (igual que antes)
afc_borrador <- MCA(df_para_afc, ncp = 5, graph = FALSE)

# 2. Extraer la calidad de representación (cos2) (igual que antes)
var_results_afc <- get_mca_var(afc_borrador)

# 3. Sumar el cos2 de las variables en las primeras 5 dimensiones (igual que antes)
cos2_sum_afc <- rowSums(var_results_afc$cos2[, 1:5])

# 4. Define un umbral (igual que antes)
umbral_afc <- 0.3 

# 5. Obtener el vector de dummies (igual que antes)
vars_cat_fuertes_DUMMIES <- names(cos2_sum_afc[cos2_sum_afc > umbral_afc])

# 6. ¡AQUÍ ESTÁ LA CORRECCIÓN!
# Usamos sub("_.+$", ...) para quitar el PRIMER guion bajo y todo lo que sigue
vars_cat_fuertes_ORIGINALES <- unique(sub("_.+$", "", vars_cat_fuertes_DUMMIES))
```

```{r}
vars_num_fuertes <- c(
  "ocultar_emociones", "dificultad_pedir_ayuda", "sin_motivacion", "preocupado",            
  "sin_proposito", "no_reconocer_logros", "incapaz_solucionar", "intento_autolesion",    
  "plan_autolesion", "autolesion_fisica", "conduccion_temeraria", "soledad",               
  "conflicto_familiar", "EIS_Roberts"
)

# 2. Crear el dataframe final para el FAMD (Este código ahora funcionará)
df_para_famd <- df %>%
  select(
    all_of(vars_num_fuertes),           # El grupo numérico fuerte
    all_of(vars_cat_fuertes_ORIGINALES) # El grupo categórico fuerte (limpio)
  ) %>%
  mutate(across(all_of(vars_cat_fuertes_ORIGINALES), as.factor)) # Asegura que sean factores
```

```{r}
vars_num_fuertes <- c(
  "ocultar_emociones", "dificultad_pedir_ayuda", "sin_motivacion", "preocupado",            
  "sin_proposito", "no_reconocer_logros", "incapaz_solucionar", "intento_autolesion",    
  "plan_autolesion", "autolesion_fisica", "conduccion_temeraria", "soledad",               
  "conflicto_familiar", "EIS_Roberts"
)

# 2. Crear el dataframe final para el FAMD
df_para_famd <- df %>%
  select(
    all_of(vars_num_fuertes),  # El grupo numérico fuerte
    all_of(vars_cat_fuertes)   # El grupo categórico fuerte
  ) %>%
  mutate(across(all_of(vars_cat_fuertes), as.factor)) # Asegura que sean factores
```


# Datos Guardados

```{r}
write.csv(df_final, "Data/df_final.csv", row.names = FALSE)
```






